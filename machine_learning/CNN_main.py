import numpy as np
from nnAudio import Spectrogram
import torch
from torch import nn
from scipy.io import wavfile
from midiutil import MIDIFile

device = "cpu"


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        # Getting Mel Spectrogram on the fly
        self.spec_layer = Spectrogram.MelSpectrogram(sr=44100, n_mels=192, hop_length=441,
                                                     window='hann', center=True, pad_mode='reflect',
                                                     fmin=20, fmax=4200, norm=1,
                                                     trainable_mel=False, trainable_STFT=False).to(device)
        k_out = 8
        k2_out = 16
        self.CNN_freq_kernel_size = (64, 1)
        self.CNN_freq_kernel_stride = (4, 1)
        self.CNN_freq = nn.Conv2d(1, k_out,
                                  kernel_size=self.CNN_freq_kernel_size, stride=self.CNN_freq_kernel_stride,
                                  padding=(2, 0))
        self.CNN_time = nn.Conv2d(k_out, k2_out,
                                  kernel_size=(1, 6), stride=(1, 1), padding=(0, 2))
        self.linear = torch.nn.Linear(2992, 748, bias=False)
        self.linear2 = torch.nn.Linear(748, 88, bias=False)

    def forward(self, x):
        z = self.spec_layer(x)

        z = torch.log(z + 1e-20)

        z2 = torch.relu(self.CNN_freq(z.unsqueeze(1)))

        z3 = torch.relu(torch.flatten(z2, 1))
        z4 = self.linear(z3)
        y = self.linear2(z4)
        return torch.sigmoid(y)


# For now just takes in an argument and returns the argument (String)

def CNN_pred(argv):
    file_path = argv
    model = Model()
    checkpoint = torch.load('./machine_learning/model/CNN_model.pt', map_location='cpu')
    model.load_state_dict(checkpoint['state_dict'], strict=False)
    model.eval()

    pred = []

    sr, song = wavfile.read(file_path)  # Loading your audio
    song = song / abs(max(song.min(), song.max(), key=abs))

    segs = len(song) // 4410

    for j in range(segs - 1):  # lazy
        seg = song[j * 4410:(j + 1) * 4410]
        x = seg.mean(1)  # Converting Stereo to Mono
        x = torch.tensor(x, device=device, dtype=torch.float)  # casting the array into a PyTorch Tensor
        y_pred = model(x)
        to_numpy = y_pred.cpu().detach().numpy()[0]
        pred.append(to_numpy)

    pred = np.round(np.array(pred))
    pred = pred.T

    # 88 is the ðŸŽ¹ ðŸ”‘  (21 to 108)
    predT = pred.T
    new_notes = []
    for i in predT:
        x = np.where(i != 0)[0] + 21
        new_notes.append(x.tolist())

    degrees = new_notes  # MIDI note number
    track = 0
    channel = 0
    time = 0.1  # In beats
    duration = 1  # In beats
    tempo = 60  # In BPM
    volume = 100  # 0-127, as per the MIDI standard

    MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track is created
    # automatically)
    MyMIDI.addTempo(track, time, tempo)

    for i, pitches in enumerate(degrees):
        if pitches != []:
            for pitch in pitches:
                MyMIDI.addNote(track, channel, pitch, time * i, duration, volume)

    with open("./outputtest.midi", "wb") as output_file:
        MyMIDI.writeFile(output_file)

    return True
